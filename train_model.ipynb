{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3230ba1b-024e-4d54-aa2d-ceb6349a93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kartik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kartik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kartik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess a single journal entry\n",
    "def preprocess_entry(entry):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(entry)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Function to get synonyms of a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to assign core value to preprocessed journal entry\n",
    "def assign_core_value(entry, core_values_keywords):\n",
    "    # Initialize dictionary to store keyword frequency for each core value\n",
    "    keyword_freq = {core_value: 0 for core_value in core_values_keywords.keys()}\n",
    "    \n",
    "    # Count frequency of core value keywords and their synonyms in the entry\n",
    "    for word in entry:\n",
    "        for core_value, keywords in core_values_keywords.items():\n",
    "            if word in keywords:\n",
    "                keyword_freq[core_value] += 1\n",
    "            else:\n",
    "                # Check if word has any synonyms in core value keywords\n",
    "                synonyms = get_synonyms(word)\n",
    "                for syn in synonyms:\n",
    "                    if syn in keywords:\n",
    "                        keyword_freq[core_value] += 1\n",
    "    \n",
    "    # Get core value with maximum keyword frequency\n",
    "    max_core_value = max(keyword_freq, key=keyword_freq.get)\n",
    "    \n",
    "    return max_core_value\n",
    "\n",
    "# Read journal entries from the text file\n",
    "with open('journal_entries.txt', 'r') as file:\n",
    "    journal_entries = file.readlines()\n",
    "    journal_entries = [entry.strip() for entry in journal_entries]\n",
    "\n",
    "# Read core values and their keywords from the text file\n",
    "with open('core_values_keywords.txt', 'r') as file:\n",
    "    core_values_keywords = eval(file.read())\n",
    "\n",
    "# Preprocess all journal entries\n",
    "preprocessed_entries = [preprocess_entry(entry) for entry in journal_entries]\n",
    "\n",
    "# Flatten preprocessed entries into sentences\n",
    "flattened_entries = [' '.join(entry) for entry in preprocessed_entries]\n",
    "\n",
    "# Convert assigned core values to numerical labels\n",
    "labels = np.array([assign_core_value(entry, core_values_keywords) for entry in preprocessed_entries])\n",
    "\n",
    "# Vectorize the text data using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(flattened_entries)\n",
    "\n",
    "# Train the Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X, labels)\n",
    "\n",
    "# Save the trained model and vectorizer for later use\n",
    "import joblib\n",
    "joblib.dump(svm_classifier, 'svm_model.joblib')\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "print(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef766eb-9e16-466d-a3a0-172c094ee3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
